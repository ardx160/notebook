{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'C:\\\\Users\\\\ardo\\\\Documents\\\\semai\\\\dataset\\\\ripeness\\\\ripeness_v8.0'\n",
    "print(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-homeless",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255, \n",
    "    validation_split=0.2)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE, \n",
    "    subset='training')\n",
    "\n",
    "val_generator = datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE, \n",
    "    subset='validation')\n",
    "\n",
    "for image_batch, label_batch in train_generator:\n",
    "  break\n",
    "image_batch.shape, label_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train_generator.class_indices)\n",
    "\n",
    "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
    "\n",
    "with open('labels_ripe.txt', 'w') as f:\n",
    "  f.write(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                              include_top=False, \n",
    "                                              weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-tennis",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.GlobalAveragePooling2D(),\n",
    "  tf.keras.layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit(train_generator, \n",
    "                    steps_per_epoch=len(train_generator), \n",
    "                    epochs=epochs, \n",
    "                    validation_data=val_generator, \n",
    "                    validation_steps=len(val_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "missing-bandwidth",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-bearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "# Fine tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer = tf.keras.optimizers.Adam(1e-5),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-interface",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "\n",
    "history_fine = model.fit(train_generator, \n",
    "                         steps_per_epoch=len(train_generator), \n",
    "                         epochs=epochs, \n",
    "                         validation_data=val_generator, \n",
    "                         validation_steps=len(val_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_fine.history['accuracy']\n",
    "val_acc = history_fine.history['val_accuracy']\n",
    "\n",
    "loss = history_fine.history['loss']\n",
    "val_loss = history_fine.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_dir = 'save/fine_tuning'\n",
    "tf.saved_model.save(model, saved_model_dir)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "with open('model.tflite', 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-creator",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 224\n",
    "img_width = 224\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=135,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    "  shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-clause",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  data_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=135,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    "  shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-kitty",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-minimum",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-draft",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalization_layer = layers.experimental.preprocessing.Rescaling(1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-outdoors",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixels values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-sucking",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n",
    "                                                 input_shape=(img_height, \n",
    "                                                              img_width,\n",
    "                                                              3)),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "    layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-murray",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    augmented_images = data_augmentation(images)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-groove",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "\n",
    "model = Sequential([\n",
    "  data_augmentation,\n",
    "  normalization_layer,\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.3),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(256, activation='relu'),\n",
    "  layers.Dropout(0.6),\n",
    "  layers.Dense(num_classes, activation='relu'),\n",
    "  layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-richardson",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stop_cuy = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=100, verbose=0,\n",
    "    mode='auto', baseline=None, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(\n",
    "        lr=0.0005\n",
    "    ),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "epochs = 1000\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    "  callbacks=[early_stop_cuy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-sterling",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(255)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-sacrifice",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tandan_url = \"https://akacn.ac.id/wp-content/uploads/2020/11/tandon-kosong-1.png\"\n",
    "tandan_loc = \"C:\\\\Users\\\\ardo\\\\Documents\\\\personal\\\\dev\\\\notebook\\\\test_ripe_0604\\\\tandan_kos.jpg\"\n",
    "tandan_path = tf.keras.utils.get_file(tandan_loc, origin=tandan_url)\n",
    "\n",
    "test_img = \"C:\\\\Users\\\\ardo\\\\Documents\\\\semai\\\\dataset\\\\ripeness\\\\ripeness_v8.0\\\\lewat matang\\\\download--2-_jpg.rf.0a6fe1263e7000eaa81c5b8b6c3a8a90.jpg\"\n",
    "\n",
    "img = keras.preprocessing.image.load_img(\n",
    "    test_img, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(predictions)\n",
    "print(class_names)\n",
    "print(np.round(score, 3))\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.1f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-bailey",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if error, delete folder cache from latest downloaded\n",
    "mobilenet_url = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
    "feature_extractor = hub.KerasLayer(mobilenet_url,\n",
    "                                   input_shape=(img_height, img_width, 3))\n",
    "\n",
    "feature_extractor.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-yukon",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 4\n",
    "\n",
    "model = Sequential([\n",
    "    data_augmentation,\n",
    "    #normalization_layer,\n",
    "    feature_extractor,\n",
    "    #layers.Dense(256, activation='relu'),\n",
    "    #layers.Dropout(0.7),\n",
    "    #layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.7),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "    #layers.Dense(num_classes)\n",
    "    #layers.ReLU(max_value=1.0)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dense-responsibility",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "early_stop_cuy = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=50, verbose=0,\n",
    "    mode='auto', baseline=None, restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(\n",
    "        lr=0.004\n",
    "  ),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "epochs = 1000\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs,\n",
    "  callbacks=[early_stop_cuy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-saudi",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# change as epoch stopped\n",
    "epochs_range = range(131)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-chaos",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tandan_url = \"https://docplayer.info/docs-images/70/62137124/images/31-0.jpg\"\n",
    "tandan_loc = \"C:\\\\Users\\\\ardo\\\\Documents\\\\personal\\\\dev\\\\notebook\\\\test_ripe_0604\\\\tandan_lewat.jpg\"\n",
    "tandan_path = tf.keras.utils.get_file(tandan_loc, origin=tandan_url)\n",
    "\n",
    "test_img = \"C:\\\\Users\\\\ardo\\\\Documents\\\\semai\\\\dataset\\\\ripeness\\\\ripeness_v7.1\\\\belum matang\\\\4_jpg.rf.6da018ee5194f730fa76294e2ea00131.jpg\"\n",
    "\n",
    "img = keras.preprocessing.image.load_img(\n",
    "    tandan_path, target_size=(img_height, img_width)\n",
    ")\n",
    "img_array = keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(predictions)\n",
    "print(class_names)\n",
    "print(np.round(score, 3))\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.1f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "optical-newark",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timename = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "saved_model_path = \"ripeness-\" + timename + \".h5\" # or you can simply use 'my_model.h5'\n",
    "model.save(saved_model_path) #save your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "involved-suicide",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 5, 32)          368672    \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 2,626,788\n",
      "Trainable params: 2,231,396\n",
      "Non-trainable params: 395,392\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# to reload your model if no snippet keras layer e.g MobileNetV2\n",
    "model = keras.models.load_model(saved_model_path)\n",
    "\n",
    "#model = keras.models.load_model(saved_model_path, custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "#model = keras.models.load_model('ripeness-20210409151225.h5', custom_objects={'KerasLayer':hub.KerasLayer})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "acknowledged-lightweight",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 37ms/step - loss: 0.4647 - accuracy: 0.8120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46468085050582886, 0.811965823173523]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_generator, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-subsection",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(val_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "offshore-exhibit",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation batch shape: (32, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>belum matang</th>\n",
       "      <th>jankos</th>\n",
       "      <th>lewat matang</th>\n",
       "      <th>matang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.0419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2176</td>\n",
       "      <td>0.7824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   belum matang  jankos  lewat matang  matang\n",
       "0        0.0000     0.0        0.0002  0.9998\n",
       "1        0.9925     0.0        0.0074  0.0001\n",
       "2        0.9792     0.0        0.0203  0.0005\n",
       "3        0.0187     0.0        0.9394  0.0419\n",
       "4        0.0000     0.0        0.2176  0.7824"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get images and labels batch from validation dataset generator\n",
    "val_image_batch, val_label_batch = next(iter(val_generator))\n",
    "print(\"Validation batch shape:\", val_image_batch.shape)\n",
    "\n",
    "# Get predictions for images batch\n",
    "tf_model_predictions = model.predict(val_image_batch)\n",
    "\n",
    "score = np.round(tf_model_predictions, 4)\n",
    "# >> Prediction results shape: (32, 5)\n",
    "\n",
    "# Convert prediction results to Pandas dataframe, for better visualization\n",
    "tf_pred_dataframe_ori = pd.DataFrame(score)\n",
    "tf_pred_dataframe_ori.columns = class_names\n",
    "\n",
    "tf_pred_dataframe_ori.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-upgrade",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get images and labels batch from validation dataset generator\n",
    "val_image_batch, val_label_batch = next(iter(val_ds))\n",
    "print(\"Validation batch shape:\", val_image_batch.shape)\n",
    "\n",
    "# Get predictions for images batch\n",
    "tf_model_predictions = model.predict(val_image_batch)\n",
    "score = tf.nn.softmax(tf_model_predictions)\n",
    "print(\"Prediction results shape:\", score.shape)\n",
    "score = np.round(score, 4)\n",
    "# >> Prediction results shape: (32, 5)\n",
    "\n",
    "# Convert prediction results to Pandas dataframe, for better visualization\n",
    "tf_pred_dataframe = pd.DataFrame(score)\n",
    "tf_pred_dataframe.columns = class_names\n",
    "\n",
    "tf_pred_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-proposal",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_pred_dataframe_ori = pd.DataFrame(tf_model_predictions)\n",
    "tf_pred_dataframe_ori.columns = class_names\n",
    "\n",
    "tf_pred_dataframe_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fabulous-stand",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lewat matang' 'matang' 'matang' 'belum matang' 'belum matang'\n",
      " 'belum matang' 'lewat matang' 'matang' 'belum matang' 'belum matang'\n",
      " 'lewat matang' 'belum matang' 'belum matang' 'lewat matang' 'matang'\n",
      " 'matang' 'matang' 'belum matang' 'matang' 'matang' 'matang'\n",
      " 'belum matang' 'lewat matang' 'matang' 'matang' 'matang' 'belum matang'\n",
      " 'belum matang' 'matang' 'lewat matang' 'matang' 'matang']\n",
      "Labels:            [[0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]]\n",
      "Predicted labels:  [2 3 3 0 0 0 2 3 0 0 2 0 0 2 3 3 3 0 3 3 3 0 2 3 3 3 0 0 3 2 3 3]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-4e57db22a0a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots_adjust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhspace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m   \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m   \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"blue\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpredicted_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"red\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m   \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_class_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m   \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGwAAABkCAYAAABjNdWgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAEMklEQVR4nO2dv28cRRiGnxeD00BB+KUIIjCSG1MBUSpahEljylClQEqFBKVR/gIoKCksEZEC4QYk3CEUIdGBAwJEsEzsIISFRRRRAA0B6aPYCTob41vf3Xjv872PtNrd7+ZuZ+/R7M3dzLeniMDk4Y6uK2AOhoUlw8KSYWHJsLBkWFgyqgmTNC9pXdKGpMVax5k0VON7mKQp4HvgWWALWAVejIjvRn6wCaNWCzsNbETE9Yi4BSwDC5WONVHUEvYw8FPP/laJmSG5s9Lrao/YjmuvpPPA+bL7dKV6ZONmRDywX4FawraAkz37jwA/9xaIiCVgCUCSf9Bs+LFfgVqXxFVgVtKMpGngLLBS6VgTRZUWFhF/S3oZ+AiYAi5GxNUax5o0qnTrD1wJXxJv80VEnNqvgH/pSIaFJcPCkmFhybCwZFhYMiwsGRaWDAtLhoUlw8KSYWHJsLBkWFgyLCwZFpYMC0uGhSXDwpJhYcnoK0zSRUk3JH3bEzsu6WNJ18r63p7HXisJEOuSnqtV8UmlTQt7B5jfFVsELkfELHC57CNpjmYO4hPlOW+VxAgzIvoKi4hPgV93hReAS2X7EvBCT3w5Iv6MiB+ADZrECDMiBv0MeygitgHK+sESdxJEZUY987dvEsS/BXcmQ5iWDNrCfpF0AqCsb5R43ySI20TEUkSc6jfT1exkUGErwLmyfQ74sCd+VtIxSTPALPD5cFU0O4iIfRfgPWAb+IumBb0E3EfTO7xW1sd7yl8ANoF14Pl+r1+eE14I4Eq/98rJEOOFkyGOGhaWDAtLhoUlw8KSYWHJsLBkWFgyLCwZFpYMC0uGhSXDwpJhYcmwsGRYWDIsLBkWlgwLS4aFJaNNMsRJSZ9IWpN0VdIrJe6EiC5oMQXtBPBU2b6H5h8f5oA3gMUSXwReL9tzwNfAMWCGZsrblKe5jWaaW5tkiO2I+LJs/w6s0cyXX8AJEYfOgT7DJD0GPAl8hhMiOqF1MoSku4H3gVcj4jdpr7yHpugesf9MFHUyxGC0amGS7qKR9W5EfFDCQyVEOBliMNr0EgW8DaxFxJs9Dzkhogta9BKfobmkfQN8VZYzjDAhgu57Z+OyOBkiGU6GOGpYWDJq/X/YQfmD5vNuErgfuPk/jz3a78njImx9Urr3kq4Mc66+JCbDwpIxLsKWuq7AITLUuY7F9zDTnnFpYaYlnQuTNF9GpjckLXZdn2E4lNH5Njc+qbXQ/APtJvA4ME0zUj3XZZ2GPJ/qo/Ndt7DTwEZEXI+IW8AyzYh1SuIQRue7FnZkR6drjc53Laz17foysXt0fr+ie8T2Pf+uhbW+XV8WaozO99K1sFVgVtKMpGma+wWvdFyngTmU0fkx6FmdoelNbQIXuq7PkOdSfXTev3Qko+tLojkgFpYMC0uGhSXDwpJhYcmwsGRYWDL+AbUmL8W3OGJLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_batch = val_image_batch.numpy()\n",
    "label_batch = val_label_batch.numpy()\n",
    "\n",
    "predicted_batch = model.predict(image_batch)\n",
    "predicted_batch = tf.squeeze(predicted_batch).numpy()\n",
    "\n",
    "predicted_ids = np.argmax(predicted_batch, axis=-1)\n",
    "\n",
    "predicted_class_names = np.array(class_names)[predicted_ids]\n",
    "\n",
    "print(predicted_class_names)\n",
    "print(\"Labels:           \", label_batch)\n",
    "print(\"Predicted labels: \", predicted_ids)\n",
    "\n",
    "plt.figure(figsize=(10,9))\n",
    "for n in range(30):\n",
    "  plt.subplot(6,5,n+1)\n",
    "  plt.subplots_adjust(hspace = 0.3)\n",
    "  plt.imshow(image_batch[n].astype('uint8'))\n",
    "  color = \"blue\" if predicted_ids[n] == label_batch[n] else \"red\"\n",
    "  plt.title(predicted_class_names[n].title(), color=color)\n",
    "  plt.axis('off')\n",
    "_ = plt.suptitle(\"Model predictions (blue: correct, red: incorrect)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-excellence",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "timename = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "tflite_path = \"ripeness-\" + timename + \".tflite\"\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "with open(tflite_path, 'wb') as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepared-springfield",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "timename = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "tflite_q_path = \"ripeness-q-\" + timename + \".tflite\"\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_q_model = converter.convert()\n",
    "with open(tflite_q_path, 'wb') as f:\n",
    "  f.write(tflite_q_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-specialist",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tflite_interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "#tflite_interpreter = tf.lite.Interpreter(model_path='ripeness-20210406165419.tflite')\n",
    "\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n\\nAfter Conversion:\\n\\n\")\n",
    "\n",
    "tflite_interpreter.resize_tensor_input(input_details[0]['index'], (32, 224, 224, 3))\n",
    "tflite_interpreter.resize_tensor_input(output_details[0]['index'], (32, 5))\n",
    "tflite_interpreter.allocate_tensors()\n",
    "\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"shape:\", output_details[0]['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-sunrise",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set batch of images into input tensor\n",
    "tflite_interpreter.set_tensor(input_details[0]['index'], val_image_batch)\n",
    "# Run inference\n",
    "tflite_interpreter.invoke()\n",
    "# Get prediction results\n",
    "tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "score_tflite = tf.nn.softmax(tflite_model_predictions)\n",
    "print(\"Prediction results shape:\", score_tflite.shape)\n",
    "score_tflite = np.round(score_tflite, 4)\n",
    "\n",
    "# Convert prediction results to Pandas dataframe, for better visualization\n",
    "tflite_pred_dataframe = pd.DataFrame(score_tflite)\n",
    "tflite_pred_dataframe.columns = class_names\n",
    "\n",
    "tflite_pred_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-kernel",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tflite_pred_dataframe_ori = pd.DataFrame(tflite_model_predictions)\n",
    "tflite_pred_dataframe_ori.columns = class_names\n",
    "\n",
    "tflite_pred_dataframe_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-advocate",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tflite_q_interpreter = tf.lite.Interpreter(model_path=tflite_q_path)\n",
    "#tflite_q_interpreter = tf.lite.Interpreter(model_path='ripeness-q-20210406165441.tflite')\n",
    "\n",
    "input_q_details = tflite_q_interpreter.get_input_details()\n",
    "output_q_details = tflite_q_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"shape:\", input_q_details[0]['shape'])\n",
    "print(\"type:\", input_q_details[0]['dtype'])\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"shape:\", output_q_details[0]['shape'])\n",
    "print(\"type:\", output_q_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n\\nAfter Conversion:\\n\\n\")\n",
    "\n",
    "tflite_q_interpreter.resize_tensor_input(input_q_details[0]['index'], (32, 224, 224, 3))\n",
    "tflite_q_interpreter.resize_tensor_input(output_q_details[0]['index'], (32, 5))\n",
    "tflite_q_interpreter.allocate_tensors()\n",
    "\n",
    "input_q_details = tflite_q_interpreter.get_input_details()\n",
    "output_q_details = tflite_q_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"shape:\", input_q_details[0]['shape'])\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"shape:\", output_q_details[0]['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-fifty",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set batch of images into input tensor\n",
    "tflite_q_interpreter.set_tensor(input_q_details[0]['index'], val_image_batch)\n",
    "# Run inference\n",
    "tflite_q_interpreter.invoke()\n",
    "# Get prediction results\n",
    "tflite_q_model_predictions = tflite_q_interpreter.get_tensor(output_q_details[0]['index'])\n",
    "score_tflite_q = tf.nn.softmax(tflite_q_model_predictions)\n",
    "print(\"Prediction results shape:\", score_tflite_q.shape)\n",
    "score_tflite_q = np.round(score_tflite_q, 4)\n",
    "\n",
    "\n",
    "# Convert prediction results to Pandas dataframe, for better visualization\n",
    "tflite_q_pred_dataframe = pd.DataFrame(score_tflite_q)\n",
    "tflite_q_pred_dataframe.columns = class_names\n",
    "\n",
    "tflite_q_pred_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-smith",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tflite_q_pred_dataframe_ori = pd.DataFrame(tflite_q_model_predictions)\n",
    "tflite_q_pred_dataframe_ori.columns = class_names\n",
    "\n",
    "tflite_q_pred_dataframe_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-blocking",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tflite_pred_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-closer",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tflite_pred_dataframe_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noble-protocol",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_pred_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-hearts",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf_pred_dataframe_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-concert",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def converttostr(input_seq, seperator):\n",
    "   # Join all the strings in list\n",
    "   final_str = seperator.join(input_seq)\n",
    "   return final_str\n",
    "\n",
    "labels = converttostr(class_names, '\\n')\n",
    "with open('labels_ripe.txt', 'w') as f:\n",
    "    f.write(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-gospel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_support import flatbuffers\n",
    "from tflite_support import metadata as _metadata\n",
    "from tflite_support import metadata_schema_py_generated as _metadata_fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automotive-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates model info.\n",
    "model_meta = _metadata_fb.ModelMetadataT()\n",
    "model_meta.name = \"ArdNetV1\"\n",
    "model_meta.description = (\"ripeness model of SeMAI\")\n",
    "model_meta.version = \"v1\"\n",
    "model_meta.author = \"Bardo Wenang\"\n",
    "model_meta.license = (\"SeMAI License\")\n",
    "\n",
    "# Creates input info.\n",
    "input_meta = _metadata_fb.TensorMetadataT()\n",
    "\n",
    "# Creates output info.\n",
    "output_meta = _metadata_fb.TensorMetadataT()\n",
    "\n",
    "input_meta.name = \"image\"\n",
    "input_meta.description = (\n",
    "    \"Input image to be classified. The expected image is {0} x {1}, with \"\n",
    "    \"three channels (red, blue, and green) per pixel. Each value in the \"\n",
    "    \"tensor is a single byte between 0 and 255.\".format(224, 224))\n",
    "input_meta.content = _metadata_fb.ContentT()\n",
    "input_meta.content.contentProperties = _metadata_fb.ImagePropertiesT()\n",
    "input_meta.content.contentProperties.colorSpace = (\n",
    "    _metadata_fb.ColorSpaceType.RGB)\n",
    "input_meta.content.contentPropertiesType = (\n",
    "    _metadata_fb.ContentProperties.ImageProperties)\n",
    "input_normalization = _metadata_fb.ProcessUnitT()\n",
    "input_normalization.optionsType = (\n",
    "    _metadata_fb.ProcessUnitOptions.NormalizationOptions)\n",
    "input_normalization.options = _metadata_fb.NormalizationOptionsT()\n",
    "input_normalization.options.mean = [127.5]\n",
    "input_normalization.options.std = [127.5]\n",
    "input_meta.processUnits = [input_normalization]\n",
    "input_stats = _metadata_fb.StatsT()\n",
    "input_stats.max = [255]\n",
    "input_stats.min = [0]\n",
    "input_meta.stats = input_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates output info.\n",
    "output_meta = _metadata_fb.TensorMetadataT()\n",
    "output_meta.name = \"probability\"\n",
    "output_meta.description = \"Probabilities of the 4 labels respectively.\"\n",
    "output_meta.content = _metadata_fb.ContentT()\n",
    "output_meta.content.content_properties = _metadata_fb.FeaturePropertiesT()\n",
    "output_meta.content.contentPropertiesType = (\n",
    "    _metadata_fb.ContentProperties.FeatureProperties)\n",
    "output_stats = _metadata_fb.StatsT()\n",
    "output_stats.max = [100.0]\n",
    "output_stats.min = [0.0]\n",
    "output_meta.stats = output_stats\n",
    "label_file = _metadata_fb.AssociatedFileT()\n",
    "label_file.name = os.path.basename(\"labels_ripe.txt\")\n",
    "label_file.description = \"Labels for objects that the model can recognize.\"\n",
    "label_file.type = _metadata_fb.AssociatedFileType.TENSOR_AXIS_LABELS\n",
    "output_meta.associatedFiles = [label_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates subgraph info.\n",
    "subgraph = _metadata_fb.SubGraphMetadataT()\n",
    "subgraph.inputTensorMetadata = [input_meta]\n",
    "subgraph.outputTensorMetadata = [output_meta]\n",
    "model_meta.subgraphMetadata = [subgraph]\n",
    "\n",
    "b = flatbuffers.Builder(0)\n",
    "b.Finish(\n",
    "    model_meta.Pack(b),\n",
    "    _metadata.MetadataPopulator.METADATA_FILE_IDENTIFIER)\n",
    "metadata_buf = b.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-sessions",
   "metadata": {},
   "outputs": [],
   "source": [
    "populator = _metadata.MetadataPopulator.with_model_file(tflite_path)\n",
    "populator.load_metadata_buffer(metadata_buf)\n",
    "populator.load_associated_files([\"labels_ripe.txt\"])\n",
    "populator.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-vermont",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
